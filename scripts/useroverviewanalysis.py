# -*- coding: utf-8 -*-
"""UserOverviewAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I3mDv95t0u0HQA_svNcy24Kh3xxTHwCx

**Data Cleaning is one of the essential step towards making real meaning of any dataset. It makes the data ready for modelling and analysis**.

Here you will be learning how to do data cleaning.
"""

import pandas as pd
import numpy as np
from IPython.display import Image

from google.colab import drive
drive.mount('/content/drive')

"""# **Identify datasets with NaN or None values**"""

import warnings
warnings.filterwarnings('ignore')
pd.set_option('max_column', None)
db = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/Week1_challenge_data_source(CSV).csv', na_values=['undefined','?', None])
db.head() # the fisrt five rows

"""# Size of the dataset
### Columns of the dataset
"""

# column names
db.columns.tolist()

"""### Number of columns"""

print(f"Number of columns: ", len(db.columns))

"""### Number of data points"""

print(f" There are {db.shape[0]} rows and {db.shape[1]} columns")

"""### Features/columns and their data type"""

db.dtypes

"""## Handling Missing Values"""

# how many missing values exist or better still what is the % of missing values in the dataset?
def percent_missing(df):

    # Calculate total number of cells in dataframe
    totalCells = np.product(df.shape)

    # Count number of missing values per column
    missingCount = df.isnull().sum()

    # Calculate total number of missing values
    totalMissing = missingCount.sum()

    # Calculate percentage of missing values
    print("The dataset contains", round(((totalMissing/totalCells) * 100), 3), "%", "missing values.")

percent_missing(db)

"""### Columns with missing values

The Column "Nb of sec with 37500B < Vol UL" has maximum missing values of 130254 occurances
"""

db.isna().sum() # missing values of each column

print ("Maximum missing values per column: ", np.max(db.isna().sum())) # print(db.isna().sum().max())

"""### maximum values of each column"""

db.max()

"""### Top 10 Handsets used"""

db_hndset_count = db['Handset Type'].value_counts()
top_10_hndsets = db_hndset_count.head(10)
print("Most used handset types in Descending order:\n", db_hndset_count)
print("\n\nTop 10 handsets used: \n", top_10_hndsets)
# i, r = pd.factorize(db['Handset Type'])
# a = np.argsort(-np.bincount(i)[i], kind='mergesort')
# db.iloc[a]
# db_sort_handset = db.sort_values('Handset Type',ascending = False) #groupby('pidx')
# db_sort_handset.head(10)

"""### top 3 handset manufacturers"""

db_hndset_manufac_count = db['Handset Manufacturer'].value_counts()
top_3_manufact = db_hndset_manufac_count.head(3)
print("Dominant manufacturers in descending order:\n", db_hndset_manufac_count)
print("\n\nTop 3 manufacturers: \n", top_3_manufact)

"""### Manufacturer-Handset pairs"""

db_hndset_manufac_pair = db.value_counts(["Handset Manufacturer", "Handset Type"])
top_3_manufact_5_hndset = db_hndset_manufac_pair.head(3)
print("Manufacturers-handset pair:\n", top_3_manufact_5_hndset)
# print("\n\nTop 3 manufacturers: \n", top_3_manufact)
# db_manufac_hndset = db.groupby("Handset Manufacturer", "Handset Type")

db['Bearer Id'].value_counts() # Each BearerID occurances aggregated

# db.value_counts('Bearer Id')

"""### user (IMEI) Grouped and Agregated with Bearer Id(xDR session)"""

# db_user_xDR = db.groupby(["IMEI","Bearer Id"]).agg(session_count = ('Bearer Id', 'value_counts')) # it also works
db_user_xDR = db.groupby(["IMEI","Bearer Id"]).size()
db_user_xDR

"""### User(IMEI) Grouped and Aggregated with session duration"""

# db_user_Duration = db.groupby(["IMEI","Dur. (ms)"]).size()
# db_user_Duration = pd.pivot_table(db, values = "Dur. (ms)",index=["IMEI"],aggfunc=np.sum)
db_user_Duration = db.groupby(["IMEI","Dur. (ms)"]).size() #transform(sum)
db_user_Duration

"""### User(IMEI) and Total UL(Upload) Grouped and Aggregated"""

db_user_UL_data = db.groupby(["IMEI","Total UL (Bytes)"]).size()
db_user_UL_data

"""### User(IMEI) and total download(DL) grouped and aggregated"""

db_user_DL_data = db.groupby(["IMEI","Total DL (Bytes)"]).size()
db_user_DL_data

"""### Data Volume for Social Media DL (Bytes)"""

db_user_DL_social_media = db.groupby(["IMEI","Social Media DL (Bytes)"]).size()
db_user_DL_social_media

"""### Data volume for Social Media UL (Bytes)"""

db_user_UL_social_media = db.groupby(["IMEI","Social Media UL (Bytes)"]).size()
db_user_UL_social_media

"""### Data volume for YouTube DL (Bytes)"""

db_user_DL_Youtube = db.groupby(["IMEI","Youtube DL (Bytes)"]).size()
db_user_DL_Youtube

"""### Data volume for YouTube UL (Bytes)"""

db_user_UL_Youtube = db.groupby(["IMEI","Youtube UL (Bytes)"]).size()
db_user_UL_Youtube

"""### Data volume for Netflix DL (Bytes)"""

db_user_DL_Netflix = db.groupby(["IMEI","Netflix DL (Bytes)"]).size()
db_user_DL_Netflix

"""### Data volume for Netflix UL (Bytes)"""

db_user_UL_Netflix = db.groupby(["IMEI","Netflix UL (Bytes)"]).size()
db_user_UL_Netflix

"""### Data volume for Google DL (Bytes)"""

db_user_DL_Google = db.groupby(["IMEI","Google DL (Bytes)"]).size()
db_user_DL_Google

"""### Data volume for Google UL (Bytes)"""

db_user_UL_Google = db.groupby(["IMEI","Google UL (Bytes)"]).size()
db_user_UL_Google

"""### Data volume for Email DL (Bytes)"""

db_user_DL_Email = db.groupby(["IMEI","Email DL (Bytes)"]).size()
db_user_DL_Email

"""### Data volume for Email UL (Bytes)"""

db_user_UL_Email = db.groupby(["IMEI","Email UL (Bytes)"]).size()
db_user_UL_Email

"""### Data volume for Gaming DL (Bytes)"""

db_user_DL_Gaming = db.groupby(["IMEI","Gaming DL (Bytes)"]).size()
db_user_DL_Gaming

"""### Data volume for Gaming UL (Bytes)"""

db_user_UL_Gaming = db.groupby(["IMEI","Gaming UL (Bytes)"]).size()
db_user_UL_Gaming

"""### Data volume for Other DL"""

db_user_DL_Other = db.groupby(["IMEI","Other DL (Bytes)"]).size()
db_user_DL_Other

"""### Data Volume for Other UL"""

db_user_UL_Other = db.groupby(["IMEI","Other UL (Bytes)"]).size()
db_user_UL_Other

"""Fixing Missing values is a crucial part of any data science/ML project because you might be making the data better by your method or otherwise. So your decision has to be perfect or close enough.

The rule of thumb is for all object datatype kind of column/features use the mode method to fill the missing datapoints and for number kind of features use the mean/median method.

The question now is how to choose which method to fill a number feature with. Well the simple answer is check if it is skewed. Now what is skew?

> In a normal distribution, the mean divides the curve symmetrically into two equal parts at the median and the value of skewness is zero. The data on the left will be equal to the data on the right side. 
 
> When a distribution is asymmetrical the tail of the distribution is skewed to one side-to the right or to the left.

> When the value of the skewness is negative, the tail of the distribution is longer towards the left hand side of the curve. In this type of distribution the mean, median, and mode of the distribution are negative rather than positive or zero.

> When the value of the skewness is positive, the tail of the distribution is longer towards the right hand side of the curve. In this type of distribution the mean, median, and mode of the distribution are positive rather than negative or zero.

Back to the rule of thumb, if the data is not skewed, filling with either mean or median will work well. But if it is indeed skew, then fill with Median. The idea is ....

Tip:
 There are different ways to look/measure the skewness of a data in pandas dataframe:
 One way is to calculate the skewness of a column:
 

```
df.skew(axis=0)
```
"""

db.skew(axis=0)

"""### Skewness visualization with histogram"""

db['Total UL (Bytes)'].hist()

db['Total DL (Bytes)'].hist()

db['Total UL (Bytes)'].hist()

"""### Positively skewed parameter"""

db['HTTP DL (Bytes)'].hist()

"""### Negatively skewwed parameter

"""

db['UL TP < 10 Kbps (%)'].hist()

"""Other method of dealing with missing values exists like fill-forward,backward-fill etc

You also need to consider what the column represent before you fill missing values for example:  ...

Another easy way to do this is to just drop all misisng rows and moving on with analysis if you have data left atleast.
"""

db.isna().sum()

db.dtypes

# # drop columns with more than 30% missing values
# df_clean = db.drop(['weight', 'payer_code', 'medical_specialty', 'max_glu_serum', 'A1Cresult'], axis=1)
# df_clean.shape

# fill missing with ffill method for columns (diag_1, diag_2, diag_3)

def fix_missing_ffill(df, col):
    df[col] = df[col].fillna(method='ffill')
    return df[col]


def fix_missing_bfill(df, col):
    df[col] = df[col].fillna(method='bfill')
    return df[col]

# fill numeric columns with ffill and bfill
db['Nb of sec with 6250B < Vol UL < 37500B'] = fix_missing_ffill(db, 'Nb of sec with 6250B < Vol UL < 37500B')
db['Nb of sec with 37500B < Vol UL'] = fix_missing_ffill(db, 'Nb of sec with 37500B < Vol UL')

# fill  object columns with mode 
db['Handset Manufacturer'] = db['Handset Manufacturer'].fillna(db['Handset Manufacturer'].mode()[0])
db['Handset Type'] = db['Handset Type'].fillna(db['Handset Type'].mode()[0])

db['Handset Type']

"""It is also possible to fillna with interploate method. Fill NaN values using an interpolation method. Based on the approach/method used, Interpolate will try to determine any missing values.

df.interpolate(inplace=True)
"""

db.interpolate(inplace=True)
db

"""## Transforming Data

**Scaling and Normalization**

Scaling vs. Normalization: What's the difference?

One of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:

    - in scaling, you're changing the range of your data, while
    - in normalization, you're changing the shape of the distribution of your data.

Scaling

This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points are, like support vector machines (SVM) or k-nearest neighbors (KNN). With these algorithms, a change of "1" in any numeric feature is given the same importance.

For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).
"""

# from sklearn.preprocessing import MinMaxScaler

# minmax_scaler = MinMaxScaler()

# numeric_columns = []
# for column in db.columns for db[column].dtypes:
#   if column.dtypes == 'float64':
#     numeric_columns.append(column)
# print(numeric_columns)
# # generate 1000 data points randomly drawn from an exponential distribution
# total_UL_data = pd['Total UL (Bytes)']#.DataFrame(np.random.exponential(200, size=2000))

# # total_UL_data.sample(5)

# db[1].min(), db[1].max()

# from matplotlib import pyplot as plt
# count, bins, ignored = plt.hist(db, 14)
# plt.show()

# mix-max scale the data between 0 and 1
# def scaler(df):
#     scaled_data = minmax_scaler.fit_transform(df)

#     # plot both together to compare
#     fig, ax = plt.subplots(1,2, figsize=(10, 6))
#     sns.histplot(original_data, ax=ax[0])
#     ax[0].set_title("Original Data")
#     sns.histplot(scaled_data, ax=ax[1])
#     ax[1].set_title("Scaled data")
    
# scaler(original_data)

"""Normalization

Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.

    Normal distribution: Also known as the "bell curve", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.

In general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with "Gaussian" in the name probably assumes normality.)

The method you will be using to normalize here is called the Normalizer method from sklearn. Let's take a quick peek at what normalizing some data looks like:
"""

# from sklearn.preprocessing import Normalizer

# def normalizer(df):
#     norm = Normalizer()
#     # normalize the exponential data with boxcox
#     normalized_data = norm.fit_transform(db)

#     # plot both together to compare
#     fig, ax=plt.subplots(1,2, figsize=(10, 6))
#     sns.histplot(df, ax=ax[0])
#     ax[0].set_title("Original Data")
#     sns.histplot(normalized_data[0], ax=ax[1])
#     ax[1].set_title("Normalized data")

# normalizer(db)

# check datatypes
db.info()

# def fix_age(col):
#     weight = [ x.replace( '[', '' ).replace( ')', '' ) for x in col.fillna('') ]
#     new_age = [ ( ( int(i.split('-')[0] ) + int( i.split('-')[1] ) ) / 2)  for i in weight ]
#     return weight, new_age

# df['age_group'], df_clean['fix_age'] = fix_weight(df_clean['age'])

# df_clean['diag_1'] = pd.to_numeric(df_clean['diag_1'], errors='coerce')
# df_clean['diag_2'] = pd.to_numeric(df_clean['diag_2'], errors='coerce')
# df_clean['diag_3'] = pd.to_numeric(df_clean['diag_3'], errors='coerce')

# df_clean.info()

# df_clean['fix_age'].unique()

# df_clean['age'] = [ ((int(i.split('-')[0]) + int(i.split('-')[1])) / 2)  for i in df_clean['fix_age']]

"""## Utility Functions"""

# # Function to calculate missing values by column
# def missing_values_table(df):
#     # Total missing values
#     mis_val = df.isnull().sum()

#     # Percentage of missing values
#     mis_val_percent = 100 * df.isnull().sum() / len(df)

#     # dtype of missing values
#     mis_val_dtype = df.dtypes

#     # Make a table with the results
#     mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)

#     # Rename the columns
#     mis_val_table_ren_columns = mis_val_table.rename(
#     columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})

#     # Sort the table by percentage of missing descending
#     mis_val_table_ren_columns = mis_val_table_ren_columns[
#         mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
#     '% of Total Values', ascending=False).round(1)

#     # Print some summary information
#     print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
#         "There are " + str(mis_val_table_ren_columns.shape[0]) +
#           " columns that have missing values.")

#     # Return the dataframe with missing information
#     return mis_val_table_ren_columns

# def format_float(value):
#     return f'{value:,.2f}'

# def find_agg(df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
    
#     new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
#                         sort_values(by=col_name, ascending=order)[:top]
    
#     return new_df

# def convert_bytes_to_megabytes(df, bytes_data):
#     """
#         This function takes the dataframe and the column which has the bytes values
#         returns the megabytesof that value
        
#         Args:
#         -----
#         df: dataframe
#         bytes_data: column with bytes values
        
#         Returns:
#         --------
#         A series
#     """
    
#     megabyte = 1*10e+5
#     df[bytes_data] = df[bytes_data] / megabyte
    
#     return df[bytes_data]

# pd.options.display.float_format = format_float

"""## Extracting Data"""

# db['readmitted'].value_counts()

# percent_missing(df_clean)

# missing_values_table(df_clean)

# def map_readmitted(col):
#     readmitted_map = {'NO' : 'NO', '>30': 'YES', '<30': 'YES'}
#     return col.map(readmitted_map)
# # 
# db['fix_readmitted'] = map_readmitted(db['readmitted'])

# unique encounter id
# db['change'].unique()

# db.shape

# pd.set_option('max_column', None)
# df = pd.read_excel("/content/drive/Shareddrives/10 Academy/Intensive training/Batch 6/Content-B6/Week-1/data/Week1_challenge_data_source.xlsx", engine = 'openpyxl')
# df.head()

"""### Mean and Mediam of some vital attributes"""

columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',
           'Google DL (Bytes)', 'Google UL (Bytes)','Total UL (Bytes)', 'Total DL (Bytes)']
db[columns].mean()

db[columns].median()

"""### Univariate analysis"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
sns.set_style('darkgrid')
sns.set(font_scale=1.3)

"""### Non-graphical Univariat EDA"""

db.describe()

db.info()

db.isna().sum()

db.isnull().sum()

"""### Graphical Univariate EDA"""

sns.histplot(x=columns[0], data =db)

sns.histplot(x=columns[1], data =db)

sns.histplot(x=db[columns]['Total UL (Bytes)'], data =db)

sns.histplot(x=db[columns]['Total DL (Bytes)'], data =db)

"""### Bivariate analysis

# NB: The pivot_table() method took me too long to return the plot

---
"""

# pd.pivot_table(db,columns =['Total DL (Bytes)','Social Media DL (Bytes)'])#,aggfunc=[np.sum])

sns.regplot(x='Total DL (Bytes)',y='Social Media DL (Bytes)',data=db)
# sns.countplot(x='Total DL (Bytes)',data=db) 
#boxplot, violinplot, stripplot, swarmplot, barplot also works

"""### Correlation Analysis"""

# db['Total UL (Bytes)'].corr(method='pearson')
db.corr(method='pearson')

