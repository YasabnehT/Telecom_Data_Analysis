#%%  
# -*- coding: utf-8 -*-
"""UserOverviewAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I3mDv95t0u0HQA_svNcy24Kh3xxTHwCx

## Module Imports
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_regression

from sklearn.model_selection import train_test_split
from joblib import dump,load
from sklearn.ensemble import RandomForestClassifier

from sklearn.cluster import KMeans
import sklearn.cluster as cluster
import scipy.spatial.distance as sdist
import pickle

"""### Mount Google Drive to Google Colab"""

# # from google.colab import drive
# drive.mount('/content/drive')

# """# Data Understanding

# ### **Identify datasets with NaN or None values**
# """

import warnings
warnings.filterwarnings('ignore')
# pd.set_option('max_column', None)
db = pd.read_csv('data/Week1_challenge_data_source(CSV).csv', na_values=['undefined','?', None])
db.head() # the fisrt five rows

"""# Size of the dataset
### Columns of the dataset
"""

# list of column  names
db.columns.tolist()

"""### Number of columns"""

print(f"Number of columns: ", len(db.columns))

"""### Number of data points and data size"""

print(f" There are {db.shape[0]} rows and {db.shape[1]} columns")

"""### Features/columns and their data type"""

db.dtypes

"""### Min and Max values of each column"""

db.max()

db.min()

"""### Utility Functions"""

# how many missing values exist or better still what is the % of missing values in the dataset?
def percent_missing(df):

    # Calculate total number of cells in dataframe
    totalCells = np.product(df.shape)

    # Count number of missing values per column
    missingCount = df.isnull().sum()

    # Calculate total number of missing values
    totalMissing = missingCount.sum()

    # Calculate percentage of missing values
    print("The dataset contains", round(((totalMissing/totalCells) * 100), 3), "%", "missing values.")


# Function to calculate missing values by column
def missing_values_table(df):
    # Total missing values
    mis_val = df.isnull().sum()

    # Percentage of missing values
    mis_val_percent = 100 * df.isnull().sum() / len(df)

    # dtype of missing values
    mis_val_dtype = df.dtypes

    # Make a table with the results
    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)

    # Rename the columns
    mis_val_table_ren_columns = mis_val_table.rename(
    columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})

    # Sort the table by percentage of missing descending
    mis_val_table_ren_columns = mis_val_table_ren_columns[
        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
    '% of Total Values', ascending=False).round(2)

    # Print some summary information
    print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
        "There are " + str(mis_val_table_ren_columns.shape[0]) +
          " columns that have missing values.")

    # Return the dataframe with missing information
    return mis_val_table_ren_columns

# fill missing numeric values with mean and object type values with mode
def fill_missing_values(df):
  for column in df.columns:
    if df[column].dtype == 'float64':
      df[column] = df[column].fillna(df[column].mean())
    elif df[column].dtypes == 'object':
      df[column] = df[column].fillna(df[column].mode()[0])
  return df


def format_float(value):
    return f'{value:,.2f}'

def find_agg(df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
    
    new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
                        sort_values(by=col_name, ascending=order)[:top]
    
    return new_df

def convert_bytes_to_megabytes(df, bytes_data):
    """
        This function takes the dataframe and the column which has the bytes values
        returns the megabytesof that value
        
        Args:
        -----
        df: dataframe
        bytes_data: column with bytes values
        
        Returns:
        --------
        A series
    """
    
    megabyte = 1*10e+5
    df[bytes_data] = df[bytes_data] / megabyte
    return df[bytes_data]

def fix_outlier(df, column):
    df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(),df[column])
    
    return df[column]


###################################PLOTTING FUNCTIONS###################################

def plot_hist(df:pd.DataFrame, column:str, color:str)->None:
    # plt.figure(figsize=(15, 10))
    # fig, ax = plt.subplots(1, figsize=(12, 7))
    sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()

def plot_count(df:pd.DataFrame, column:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.countplot(data=df, x=column)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()
    
def plot_bar(df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:
    plt.figure(figsize=(12, 7))
    sns.barplot(data = df, x=x_col, y=y_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.yticks( fontsize=14)
    plt.xlabel(xlabel, fontsize=16)
    plt.ylabel(ylabel, fontsize=16)
    plt.show()

def plot_heatmap(df:pd.DataFrame, title:str, cbar=False)->None:
    plt.figure(figsize=(12, 7))
    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )
    plt.title(title, size=18, fontweight='bold')
    plt.show()

def plot_box(df:pd.DataFrame, x_col:str, title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.boxplot(data = df, x=x_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.show()

def plot_box_multi(df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.boxplot(data = df, x=x_col, y=y_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()

def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
    plt.figure(figsize=(12, 7))
    sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)
    plt.title(title, size=20)
    plt.xticks(fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()


pd.options.display.float_format = format_float

percent_missing(db)

"""### Missing Value table"""

missing_values_table(db)

"""### Columns with missing values count

The Column "Nb of sec with 37500B < Vol UL" has maximum missing values of 130254 occurances
"""

db.isna().sum() # missing values of each column

print ("Maximum missing values per column: ", np.max(db.isna().sum())) # print(db.isna().sum().max())

"""# Exploratory Data Analysis

Use Mode method to fill the missing datapoints of all 'object' type features and Mean/Median methods for all numuric type features.
*   use Median method for skewed(negative/positive) numeric feature and 
*   use MEAN/Median for non-skewd/symetrical numeric feature

### Method selection based on data skewness

#### Skewness of each column
"""

db.skew(axis=0)

"""### Skewness visualization with histogram"""

db['Total UL (Bytes)'].hist()

"""### Positively skewed parameter"""

db['UL TP < 10 Kbps (%)'].hist()

"""### Negatively skewwed parameter"""

db['UL TP < 10 Kbps (%)'].hist()

"""### Data with total missing values in each column - revisited"""

# db.isna().sum()

"""### Fill missing values
* numeric missing values with mean method
* object type missing values with mode method
"""

fill_missing_values(db).isna().sum()

"""### Other method of handling missing values - Interpolation
* We can use interpolation while working with time-series data because in time-series data we like to fill missing values with previous one or two values.
* It can be used to estimate unknown data points between two known data points.

##### Since we are not considering the time-series nature of the telecom data, we choose not to use interpolation here.
"""

# db.interpolate(inplace=True)

# db.isna().sum()

"""# Data Transformation
**Scaling and Normalization**

##### Scaling - changing the range of your data 
##### Normalization, you're changing the shape of the distribution of your data.

#### Scaling

* This transforms data so that it fits within a specific scale, like 0-100 or 0-1. 
* It is important when we're using methods based on distance measures of data points like support vector machines (SVM) or k-nearest neighbors (KNN).
* We use the scaler method from sklearn.

#### Normalization

Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.

* Normal distribution ("bell curve", Gaussian distribution) is a specific statistical distribution where a roughly equal observations fall above and below the mean
 * The mean and the median are the same, and there are more observations closer to the mean.

* In general, you'll normalize your data if you're going to be using a machine learning or statistics technique like LDA and Gaussian naive Bayes that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with "Gaussian" in the name probably assumes normality.)

* We usee the Normalizer method from sklearn

### Numeric Value Scaling
"""
db = pd.read_csv('data/Week1_challenge_data_source(CSV).csv', na_values=['undefined','?', None])

minmax_scaler = preprocessing.MinMaxScaler()
def scalling_numeric_values(df):
  col_values = []
  col_exclude = ['Bearer Id', 'IMSI','MSISDN/Number','IMEI']
  for column in df.columns:
    if df[column].dtype == 'float64' and (column not in col_exclude):
      col_values.append(list(df[column].values))
  col_values_scaled = minmax_scaler.fit_transform(col_values)
  db_scaled = pd.DataFrame(col_values_scaled)
  return df

db_sklearn = fill_missing_values(db.copy())
scalling_numeric_values(db_sklearn)
db_sklearn

"""### Scaling between [0,1]"""

def scalling_numeric_values_0_1(df):
  for column in df.columns:
    col_exclude = ['Bearer Id', 'IMSI','MSISDN/Number','IMEI']
    if df[column].dtype == 'float64' and (column not in col_exclude):
      df[column] = MinMaxScaler().fit_transform(np.array(df[column]).reshape(150001,1))
  return df

db_sklearn = fill_missing_values(db.copy())
scalling_numeric_values_0_1(db_sklearn)

"""### Mean and Mediam of some vital attributes"""

important_columns_numeric = ['Bearer Id','Dur. (ms)','MSISDN/Number',
                      'Avg RTT DL (ms)','Avg RTT UL (ms)',
                      'TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)',
                      'Social Media DL (Bytes)', 'Social Media UL (Bytes)',
                      'Google DL (Bytes)', 'Google UL (Bytes)', 
                      'Email DL (Bytes)','Email UL (Bytes)',
                      'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
                      'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 
                      'Gaming DL (Bytes)','Gaming UL (Bytes)',
                      'Other DL (Bytes)', 'Other UL (Bytes)',
                      'Total UL (Bytes)', 'Total DL (Bytes)' ]

important_columns_object = ['Handset Manufacturer','Handset Type']

db[important_columns_numeric].mean() #mean of numeric columns

# db[important_columns_numeric].median()

db[important_columns_object].mode()

"""# User Overview Analysis

## Univariate Analysis
* Numric Univaraite Analysis
* Categorical Univariate Analysis

### Numeric Univariate EDA
"""

db_explore = db.copy()
db_explore.to_csv("data/db_explore.csv",header = True, index = False)

fix_outlier(db_explore, "MSISDN/Number")

plot_hist(db_explore.head(10000),"MSISDN/Number" ,'green')

plot_hist(db_explore, "Dur. (ms)", "green")

plot_hist(db_explore, "Bearer Id", "green")

sns.regplot(data = db_explore,x="Avg RTT DL (ms)",y="Avg RTT UL (ms)" ) #sns.barplot(,y='Social Media UL (Bytes)',data=db_explore.head(1000))

# plot_hist(db, "Avg RTT UL (ms)", "green")

plot_hist(db_explore.head(50000), "Handset Manufacturer", "blue")

plot_hist(db_explore.head(50000), "Handset Type", "blue")

# sns.regplot(data = db_encoded,x="Handset Type",y="Handset Manufacturer" ) #sns.barplot(,y='Social Media UL (Bytes)',data=db_explore.head(1000))

plot_hist(db_explore, "Social Media DL (Bytes)", "green")

# sns.histplot(x=columns[0], data =db) # this also works
plot_hist(db_explore, "Social Media UL (Bytes)", "green")

sns.regplot(data = db,x="Social Media DL (Bytes)",y="Social Media UL (Bytes)" ) #sns.barplot(,y='Social Media UL (Bytes)',data=db_explore.head(1000))

plot_hist(db_explore, "Total DL (Bytes)", "green")

plot_hist(db_explore, "Total UL (Bytes)", "green")

sns.regplot(data = db,x="Total DL (Bytes)",y="Total UL (Bytes)" ) #sns.barplot(,y='Social Media UL (Bytes)',data=db_explore.head(1000))

plot_box(db_explore, "Dur. (ms)", "Session Duration Outliers")

plot_box(db_explore, "Avg RTT DL (ms)", "Avg RTT DL (ms) Outliers")

plot_box(db_explore, "Avg RTT UL (ms)", "Avg RTT UL (ms) Outliers")

plot_box(db_explore, "TCP DL Retrans. Vol (Bytes)", "TCP DL Retrans. Vol (Bytes) Outliers")

plot_box(db_explore, "TCP UL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes) Outliers")

plot_box(db_explore, "Social Media DL (Bytes)", "Social Media DL (Bytes) Outliers")

plot_box(db_explore, "Social Media UL (Bytes)", "Social Media UL (Bytes) Outliers")

plot_box(db_explore, "Total DL (Bytes)", "Total DL (Bytes) Outliers")

plot_box(db_explore, "Total UL (Bytes)", "Total UL (Bytes) Outliers")

"""### Categorical Univariate EDA"""

plot_count(db_explore, "Handset Manufacturer")

plot_count(db_explore, "Handset Type")

"""### Non-Graphical Univariate EDA"""

db.describe()

db["Total DL (Bytes)"].describe()

db["Total UL (Bytes)"].describe()

db["MSISDN/Number"].describe()

db.info()

db.isna().sum()

db_explore_100 = db_explore.head(100)

"""## Bivariate Analysis
#### Applications Vs Total DL and Total UL
"""

sns.barplot(x='Total DL (Bytes)',y='Social Media DL (Bytes)',data=db_explore_100)

sns.barplot(x='Total DL (Bytes)',y='Social Media UL (Bytes)',data=db_explore.head(1000))

sns.barplot(x='Total DL (Bytes)',y='Social Media UL (Bytes)',data=db_explore_100)

sns.barplot(x='Total UL (Bytes)',y='Social Media DL (Bytes)',data=db_explore_100)

sns.barplot(x='Total UL (Bytes)',y='Social Media UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Google DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Google UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Google DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Google UL (Bytes)',data=db_explore_100)

sns.stripplot(x='Total DL (Bytes)',y='Email DL (Bytes)',data=db_explore_100)

sns.stripplot(x='Total DL (Bytes)',y='Email UL (Bytes)',data=db_explore_100)

sns.stripplot(x='Total UL (Bytes)',y='Email DL (Bytes)',data=db_explore_100)

sns.stripplot(x='Total UL (Bytes)',y='Email UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Youtube DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Youtube UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Youtube DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Youtube UL (Bytes)',data=db_explore_100)

sns.barplot(x='Total DL (Bytes)',y='Netflix DL (Bytes)',data=db_explore_100)

sns.barplot(x='Total DL (Bytes)',y='Netflix UL (Bytes)',data=db_explore_100)

sns.barplot(x='Total UL (Bytes)',y='Netflix DL (Bytes)',data=db_explore_100)

sns.barplot(x='Total UL (Bytes)',y='Netflix UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Gaming DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Gaming UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Gaming DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Gaming UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Other DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total DL (Bytes)',y='Other UL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Other DL (Bytes)',data=db_explore_100)

sns.regplot(x='Total UL (Bytes)',y='Other UL (Bytes)',data=db_explore_100)

"""##Multivariate Analysis"""

plot_scatter(db_explore.head(100), x_col="MSISDN/Number", y_col="Social Media DL (Bytes)", hue="Social Media UL (Bytes)",
             style="Social Media UL (Bytes)", title="Social media DL consumption per user")

plot_scatter(db_explore.head(100), x_col="MSISDN/Number", y_col="Total DL (Bytes)", hue="Total UL (Bytes)",
             style="Total UL (Bytes)", title="Total DL consumption per user")

plot_box_multi(db_explore.head(100), x_col="MSISDN/Number", y_col="TCP DL Retrans. Vol (Bytes)", 
               title="TCP DL Retrans. Vol (Bytes) outilers in MSISDN/Number column")

dfPair = db_explore.head(50)[["MSISDN/Number", "Dur. (ms)", "Avg RTT DL (ms)", "Social Media DL (Bytes)", "Total DL (Bytes)"]]

sns.pairplot(dfPair, hue = 'Total DL (Bytes)', diag_kind = 'kde',
             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
             height=4)

dfPair = db_explore.head(50)[["MSISDN/Number", "Dur. (ms)", "Avg RTT DL (ms)", "Social Media DL (Bytes)", "Total DL (Bytes)"]]
sns.pairplot(dfPair, hue = 'Total DL (Bytes)', diag_kind = 'kde',height=4)

"""# Deciles

### Decile Columns
"""

decile_columns = ['MSISDN/Number','Dur. (ms)','Total UL (Bytes)', 'Total DL (Bytes)' ] # to limit the number of columns to be displayed
db_decile = db_explore[decile_columns]
# db_decile_group["Dur. Decile"] = pd.qcut(db_decile_group['Dur. (ms)'], 5, labels = ['Dec 1','Dec 2','Dec 3','Dec 4','Dec 5'])
# db_decile_group

"""### Five MSISDN deciles based on xDR Duration
#### contains all selected columns
"""

db_decile_group_dur = db_decile.groupby(pd.qcut(db_decile["Dur. (ms)"], 5))
db_decile_group_dur.describe() # includes all selected columns

"""### Deciles based on xDR duration
### Contains only the xDR duration data
"""

db_decile_group_dur['Dur. (ms)'].describe()

"""### Decile Total DL Bytes sum



"""

db_decile_group_dur['Total DL (Bytes)'].sum()

"""### Decile Total UL Bytes sum

"""

db_decile_group_dur['Total UL (Bytes)'].sum()

"""## Correlation Analysis

### Correlation Analysis for the whole data

"""

db.corr(method='pearson')

"""### Correlation Analysis for individual columns
* Can be calculated using 'pearson’, ‘kendall’, ‘spearman methods; pearson being the standard correlation coefficient


"""

cor_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)',
               'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 
               'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)'] 
db[cor_columns].corr(method='pearson')

"""### Unapproximated pairwise Correlation coefficients"""

db[cor_columns[0]].corr(db[cor_columns[1]], method = 'pearson')

def Iterative_corr():
  for i in range(0,len(cor_columns)):
    print(f"Correlation between {cor_columns[i-1]} and {cor_columns[i]} is {db[cor_columns[i-1]].corr(db[cor_columns[i]], method = 'pearson')}")

Iterative_corr()

"""## Principal Component Analysis"""

db_explore_PCA = PCA(n_components=5)

db_explore_numeric = db_explore[important_columns_numeric]
db_explore_PC = db_explore_PCA.fit_transform(db_explore_numeric)

principal_db_explore_df = pd.DataFrame(data = db_explore_PC,
                        columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])
principal_db_explore_df.head()

print(f'Explained variation per principal component: {db_explore_PCA.explained_variance_ratio_}')

"""# User Engagement Analysis

### Top 10 Handsets used
"""

db_hndset_count = db['Handset Type'].value_counts()
top_10_hndsets = db_hndset_count.head(10)
print("Most used handset types in Descending order:\n", db_hndset_count)
print("\n\nTop 10 handsets used: \n", top_10_hndsets)

"""### Top 3 handset manufacturers"""

db_hndset_manufac_count = db['Handset Manufacturer'].value_counts()
top_3_manufact = db_hndset_manufac_count.head(3)
print("Dominant manufacturers in descending order:\n", db_hndset_manufac_count)
print("\n\nTop 3 manufacturers: \n", top_3_manufact)

"""### Manufacturer-Handset pairs"""

db_hndset_manufac_pair = db.value_counts(["Handset Manufacturer", "Handset Type"])
top_3_manufact_5_hndset = db_hndset_manufac_pair.head(3)
print("Manufacturers-handset pair:\n", top_3_manufact_5_hndset)

"""## Data Aggregation with each column

### Frequency of each session (Bearer Id)
"""

db['Bearer Id'].value_counts() # Each xDR occurances aggregated
# db.value_counts('Bearer Id') #also works

"""### 10 most frequent sessions"""

db['Bearer Id'].value_counts().head(10)

"""### Frequency of each User(MSISDN/Number)"""

db["MSISDN/Number"].value_counts()

"""### Top 10 frequent users"""

db["MSISDN/Number"].value_counts().head(10)

"""### User (MSISDN) Grouped and Agregated with Bearer Id(xDR session)
Each user has unique xDR session

### User-Session pair frequencies
"""

db_user_xDR = db.groupby(["MSISDN/Number"]).agg(session_count = ('Bearer Id', 'value_counts')).sort_values(by='session_count', ascending = False)#.value_counts(ascending = False) # it also works
# db_user_xDR = db[['MSISDN/Number', 'Bearer Id']].value_counts() # this also works
db_user_xDR

"""### Top 10 frequent users-session pairs"""

db_user_xDR = db.groupby(["MSISDN/Number"]).agg(session_count = ('Bearer Id', 'value_counts')).sort_values(by='session_count', ascending = False)#.value_counts(ascending = False) # it also works
# db_user_xDR = db[['MSISDN/Number', 'Bearer Id']].value_counts() # this also works
db_user_xDR.head(10)

"""### User(MSISDN) Grouped and Aggregated with xDR duration"""

# db_user_Duration = db.groupby(["MSISDN/Number","Dur. (ms)"]).size()
db_user_Duration = db.groupby(["MSISDN/Number"]).agg(Duration = ('Dur. (ms)', 'value_counts')).sort_values(by='Duration', ascending = False)#.value_counts(ascending = False) # it also works

db_user_Duration

"""### Top 10 frequent durations users have engaged
#### These are frequent durations when users stay connected
"""

db_user_Duration = db.groupby(["MSISDN/Number"]).agg(Duration = ('Dur. (ms)', 'value_counts')).sort_values(by='Duration', ascending = False)#.value_counts(ascending = False) # it also works

db_user_Duration.head(10)

"""### User Engagement per duration measure"""

db_user_Duration = db.groupby(["MSISDN/Number"]).agg(Duration_ms = ('Dur. (ms)', 'sum')).sort_values(by='Duration_ms', ascending = False)#.value_counts(ascending = False) # it also works

db_user_Duration

"""### Top 10 user with longer engagement durations (ms)
#### These are users who engaged more by staying connected longer
"""

db_user_Duration = db.groupby(["MSISDN/Number"]).agg(Duration_ms = ('Dur. (ms)', 'sum')).sort_values(by='Duration_ms', ascending = False)#.value_counts(ascending = False) # it also works

db_user_Duration.head(10)

"""### User(MSISDN) and Total UL(Upload) Grouped and Aggregated"""

db_user_UL_data = db.groupby(["MSISDN/Number"]).agg(Total_UL_Bytes = ("Total UL (Bytes)", 'sum')).sort_values(by='Total_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_UL_data

"""### Top 10 Users with highest Total Upload Bytes"""

db_user_UL_data = db.groupby(["MSISDN/Number"]).agg(Total_UL_Bytes = ("Total UL (Bytes)", 'sum')).sort_values(by='Total_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
# db_user_UL_data = db[['MSISDN/Number', 'Total UL (Bytes)']].sum()#value_counts() # this also works
db_user_UL_data.head(10)

"""### User(MSISDN) and total download(DL) grouped and aggregated"""

db_user_DL_data = db.groupby(["MSISDN/Number"]).agg(Total_DL_Bytes = ("Total DL (Bytes)", 'sum')).sort_values(by='Total_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_DL_data

"""### Top 10 Users with highest Total Download Bytes"""

db_user_DL_data = db.groupby(["MSISDN/Number"]).agg(Total_DL_Bytes = ("Total DL (Bytes)", 'sum')).sort_values(by='Total_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_DL_data.head(10)

"""## User-Total Bytes Aggregation"""

# Total Bytes (Total UL + Total DL)
db_Total_Bytes = db.copy()
db_Total_Bytes["Total Bytes"] = db["Total DL (Bytes)"] + db["Total UL (Bytes)"]

# Total Social Media Bytes
db_Total_Bytes["Total Social Media (Bytes)"] = db["Social Media DL (Bytes)"] + db["Social Media UL (Bytes)"]

#Total Google Bytes
db_Total_Bytes["Total Google (Bytes)"] = db["Google DL (Bytes)"] + db["Google UL (Bytes)"]

#Total Youtube Bytes
db_Total_Bytes["Total Youtube (Bytes)"] = db["Youtube DL (Bytes)"] + db["Youtube UL (Bytes)"]

#Total Email Bytes
db_Total_Bytes["Total Email (Bytes)"] = db["Email DL (Bytes)"] + db["Email UL (Bytes)"]

#Total Netflix Bytes
db_Total_Bytes["Total Netflix (Bytes)"] = db["Netflix DL (Bytes)"] + db["Netflix UL (Bytes)"]

#Total Gaming Bytes
db_Total_Bytes["Total Gaming (Bytes)"] = db["Gaming DL (Bytes)"] + db["Gaming UL (Bytes)"]

#Total Other Bytes
db_Total_Bytes["Total Other (Bytes)"] = db["Other DL (Bytes)"] + db["Other UL (Bytes)"]


### User Experience ######
# Total TCP Retransmission Bytes
db_Total_Bytes["Total TCP Retrans. (Bytes)"] = db['TCP DL Retrans. Vol (Bytes)'] + db['TCP UL Retrans. Vol (Bytes)']

# Total Avg RTT 
db_Total_Bytes["Total Avg RTT (ms)"] = db['Avg RTT DL (ms)'] + db['Avg RTT UL (ms)']

# Total Avg Bearer TP
db_Total_Bytes["Total Avg TP (kbps)"] = db['Avg Bearer TP DL (kbps)'] + db['Avg Bearer TP DL (kbps)']

db_totals_col = db_Total_Bytes[["MSISDN/Number",'Dur. (ms)', "Handset Type", "Total Bytes", "Total TCP Retrans. (Bytes)", 
                "Total Avg RTT (ms)", "Total Avg TP (kbps)","Total Social Media (Bytes)", 
               "Total Google (Bytes)", "Total Youtube (Bytes)","Total Email (Bytes)", 
               "Total Netflix (Bytes)", "Total Gaming (Bytes)", "Total Other (Bytes)"]]
db_totals_col.head()

"""### User-Total Social Media Bytes

"""

db_user_Social = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Social_Media_Bytes = ("Total Social Media (Bytes)", 'sum')).sort_values(by='Total_Social_Media_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Social

"""### Top 10 Social Media Users"""

db_user_Social.head(10)

"""### User-Total Google Bytes"""

db_user_Google = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Google_Bytes = ("Total Google (Bytes)", 'sum')).sort_values(by='Total_Google_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Google

"""### Top 10 Google Users"""

db_user_Google.head(10)

"""### User-Total Youtube Bytes"""

db_user_Youtube = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Youtube_Bytes = ("Total Youtube (Bytes)", 'sum')).sort_values(by='Total_Youtube_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Youtube

"""### Top 10 Youtube Users"""

db_user_Youtube.head(10)

"""### User-Total Email Bytes"""

db_user_Email = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Email_Bytes = ("Total Email (Bytes)", 'sum')).sort_values(by='Total_Email_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Email

"""### Top 10 Email Users"""

db_user_Email.head(10)

"""### User-Total Netflix Bytes"""

db_user_Netflix = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Netflix_Bytes = ("Total Netflix (Bytes)", 'sum')).sort_values(by='Total_Netflix_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Netflix

"""### Top 10 Netflix Users"""

db_user_Netflix.head(10)

"""### User-Total Gaming Bytes"""

db_user_Gaming = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Gaming_Bytes = ("Total Gaming (Bytes)", 'sum')).sort_values(by='Total_Gaming_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Gaming

"""### Top 10 Gaming Users"""

db_user_Gaming.head(10)

"""### User-Total Other Services Bytes"""

db_user_Other = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_Other_Bytes = ("Total Other (Bytes)", 'sum')).sort_values(by='Total_Other_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Other

"""### Top 10 Other Services Users"""

db_user_Other.head(10)

"""### User (MSISDN) aggregated with Social Media DL data volume"""

db_user_social_DL = db.groupby(["MSISDN/Number"]).agg(Social_Media_DL_Bytes = ("Social Media UL (Bytes)", 'sum')).sort_values(by='Social_Media_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_social_DL

"""### Top 10 users with largest Social Media Download"""

db_user_social_DL.head(10)

"""### Data volume for Social Media UL (Bytes)"""

db_user_social_UL = db.groupby(["MSISDN/Number"]).agg(Social_Media_UL_Bytes = ("Social Media UL (Bytes)", 'sum')).sort_values(by='Social_Media_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_social_UL

"""### Top 10 users with lasrgest Social Media Upload"""

db_user_social_UL.head(10)

"""### Data volume for YouTube DL (Bytes)"""

db_user_youtube_DL = db.groupby(["MSISDN/Number"]).agg(Youtube_DL_Bytes = ("Youtube DL (Bytes)", 'sum')).sort_values(by='Youtube_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_youtube_DL

"""### Top 10 users with largest Youtube Download"""

db_user_youtube_DL.head(10)

"""### Data volume for YouTube UL (Bytes)"""

db_user_youtube_UL = db.groupby(["MSISDN/Number"]).agg(Youtube_UL_Bytes = ("Youtube UL (Bytes)", 'sum')).sort_values(by='Youtube_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_youtube_UL

"""### Top 10 users with largest Youtube Upload"""

db_user_youtube_UL.head(10)

"""### Data volume for Netflix DL (Bytes)"""

db_user_Netflix_DL = db.groupby(["MSISDN/Number"]).agg(Netflix_DL_Bytes = ("Netflix DL (Bytes)", 'sum')).sort_values(by='Netflix_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Netflix_DL

"""### Top 10 users with largest Netflix Download"""

db_user_Netflix_DL.head(10)

"""### Data volume for Netflix UL (Bytes)"""

db_user_Netflix_UL = db.groupby(["MSISDN/Number"]).agg(Netflix_UL_Bytes = ("Netflix UL (Bytes)", 'sum')).sort_values(by='Netflix_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Netflix_UL

"""### Top 10 users with Netflix Upload"""

db_user_Netflix_UL.head(10)

"""### Data volume for Google DL (Bytes)"""

db_user_Google_DL = db.groupby(["MSISDN/Number"]).agg(Google_DL_Bytes = ("Google DL (Bytes)", 'sum')).sort_values(by='Google_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Google_DL

"""### Top 10 users with largest Google Download"""

db_user_Google_DL.head(10)

"""### Data volume for Google UL (Bytes)"""

db_user_Google_UL = db.groupby(["MSISDN/Number"]).agg(Google_UL_Bytes = ("Google UL (Bytes)", 'sum')).sort_values(by='Google_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Google_UL

"""### Top 10 users with largest Google Upload"""

db_user_Google_UL.head(10)

"""### Data volume for Email DL (Bytes)"""

db_user_Email_DL = db.groupby(["MSISDN/Number"]).agg(Email_DL_Bytes = ("Email DL (Bytes)", 'sum')).sort_values(by='Email_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Email_DL

"""### Top 10 users with largest Email Download"""

db_user_Email_DL.head(10)

"""### Data volume for Email UL (Bytes)"""

db_user_Email_UL = db.groupby(["MSISDN/Number"]).agg(Email_UL_Bytes = ("Email UL (Bytes)", 'sum')).sort_values(by='Email_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Email_UL

"""### Top 10 users with largest Email Upload"""

db_user_Email_UL.head(10)

"""### Data volume for Gaming DL (Bytes)"""

db_user_Gaming_DL = db.groupby(["MSISDN/Number"]).agg(Gaming_DL_Bytes = ("Gaming DL (Bytes)", 'sum')).sort_values(by='Gaming_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works

# db_user_DL_Gaming = db.groupby(["MSISDN/Number"]).agg({'Gaming DL (Bytes)':'sum'}) #it works but not sorted with Bytes

db_user_Gaming_DL

"""### Top 10 users with largest Gaming Download"""

db_user_Gaming_DL.head(10)

"""### Data volume for Gaming UL (Bytes)"""

db_user_Gaming_UL = db.groupby(["MSISDN/Number"]).agg(Gaming_UL_Bytes = ("Gaming UL (Bytes)", 'sum')).sort_values(by='Gaming_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Gaming_UL

"""### Top 10 users with largest Gaming Upload"""

db_user_Gaming_UL.head(10)

"""### Data volume for Other DL"""

db_user_other_DL = db.groupby(["MSISDN/Number"]).agg(Other_DL_Bytes = ("Other DL (Bytes)", 'sum')).sort_values(by='Other_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_other_DL

"""### Top 10 users with largest Other Services Download"""

db_user_other_DL.head(10)

"""### Data Volume for Other UL"""

db_user_other_UL = db.groupby(["MSISDN/Number"]).agg(Other_UL_Bytes = ("Other UL (Bytes)", 'sum')).sort_values(by='Other_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_other_UL

"""### Top 10 users with largest Other Services Download"""

db_user_other_UL.head(10)

"""#Top 10 users with largest Total Download"""

db_user_Total_DL = db.groupby(["MSISDN/Number"]).agg(Total_DL_Bytes = ("Total DL (Bytes)", 'sum')).sort_values(by='Total_DL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Total_DL.head(10)

"""### Top 10 users with largest Total Uploads"""

db_user_Total_UL = db.groupby(["MSISDN/Number"]).agg(Total_UL_Bytes = ("Total UL (Bytes)", 'sum')).sort_values(by='Total_UL_Bytes', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Total_UL.head(10)

"""### Top 3 most used applications"""

social_total = db["Social Media DL (Bytes)"].sum() + db["Social Media UL (Bytes)"].sum()

google_total = db["Google DL (Bytes)"].sum() + db["Google UL (Bytes)"].sum()
email_total = db["Email DL (Bytes)"].sum() + db["Email UL (Bytes)"].sum()
youtube_total = db["Youtube DL (Bytes)"].sum() + db["Youtube UL (Bytes)"].sum()
netflix_total = db["Netflix DL (Bytes)"].sum() + db["Netflix UL (Bytes)"].sum()
gaming_total = db["Gaming DL (Bytes)"].sum() + db["Gaming UL (Bytes)"].sum()
other_total = db["Other DL (Bytes)"].sum() + db["Other UL (Bytes)"].sum()

dict_bytes = {"Total Social Media Bytes":social_total, "Total Google Bytes":google_total,"Total Email Bytes":email_total, 
              "Total YouTube Bytes":youtube_total,"Total Netflix Bytes":netflix_total, 
              "Total Gaming Bytes": gaming_total, "Total Other Services Bytes":other_total}

max_3_keys = sorted(dict_bytes, key=dict_bytes.get, reverse=True)
for key in max_3_keys:
  print(f"{key} : {dict_bytes[key]}\n")

"""### Top 3 most used applications"""

max_3_keys = sorted(dict_bytes, key=dict_bytes.get, reverse=True)[:3]
for key in max_3_keys:
  print(f"{key} : {dict_bytes[key]}\n")

"""## K-Means clustering for each metric

### Normalize each metrics
* Frequency
* Duration of session - already normalized
* Total DL - already normalized
* Total UL -already normalized

#### Normalized Total DL
"""

db_sklearn['Total DL (Bytes)'].head()

"""#### Normalized Total UL"""

db_sklearn['Total UL (Bytes)'].head()

"""#### Normalized Duration"""

db_sklearn['Dur. (ms)'].head()

# min_max_scaler = preprocessing.MinMaxScaler()

# dur_norm = (db_sklearn['Dur. (ms)'].values).reshape(-1,1) #returns a numpy array, reshape the feature
# dur_norm_scaled = min_max_scaler.fit_transform(dur_norm)
# db_sklearn[["Dur. (ms)"]] = pd.DataFrame(dur_norm_scaled)
# db_sklearn['Dur. (ms)'].head()

"""### Clustering Functions"""

kmeans = cluster.KMeans(n_clusters = 3, init = "k-means++", max_iter = 300
                  , n_init = 10, random_state=0)
#Clustering function
def Cluster(df,cols, title:str, xlabel:str, ylabel:str):
  X = df[cols].values
  # According to the elbow method, the nuber of clusters is 3
  
  # apply fit_predict -  map which sample to which cluster
  # and return number of clusters as single vector y K-means
  y_kmeans = kmeans.fit_predict(X)

  # Visualize the clusters
  plt.scatter(X[y_kmeans==0, 0],X[y_kmeans==0,1], s=100,c='red', label= "Cluster 1")
  plt.scatter(X[y_kmeans==1, 0],X[y_kmeans==1,1], s=100,c='blue', label= "Cluster 2")
  plt.scatter(X[y_kmeans==2, 0],X[y_kmeans==2,1], s=100,c='green', label= "Cluster 3")

  # plot Centroids
  plt.scatter(kmeans.cluster_centers_[:,0],
              kmeans.cluster_centers_[:,1],s=300,c='yellow', label='Centroids')
  plt.title(title)
  plt.xlabel(xlabel)
  plt.ylabel(ylabel)
  plt.show()

  print(f"\nCentroids:\n{kmeans.cluster_centers_}\n\n")
  #Min-Max
  print(f"Min-Max Values \n\nCluster 1:\n\n")
  print(f"Max Value(ms) {X[y_kmeans==0,1].max()}\nMin Value(ms): {X[y_kmeans==0,1].min()}\nMean Value(ms):{X[y_kmeans==0,1].mean()}\nTotal Value(ms): {X[y_kmeans==0,1].sum()}\n\n")

  print(f"Cluster 2:\n\n")
  print(f"Max Value(ms) {X[y_kmeans==1,1].max()}\nMin Value(ms): {X[y_kmeans==1,1].min()}\nMean Value(ms):{X[y_kmeans==1,1].mean()}\nTotal Value(ms): {X[y_kmeans==1,1].sum()}\n\n")

  print(f"Cluster 3:\n\n")
  print(f"Max Value(ms) {X[y_kmeans==2,1].max()}\nMin Value(ms): {X[y_kmeans==2,1].min()}\nMean Value(ms):{X[y_kmeans==2,1].mean()}\nTotal Value(ms): {X[y_kmeans==2,1].sum()}\n\n")
  
  print("Optimal n_clusters - Elbow Method\n\n")
  n_clusters_elbow(X)


# elbow method to select n_clusters
def n_clusters_elbow(X):
  # uses within cluster sum of squares, WCSS
  wcss = []
  #fit kmeans to data and compute wcss
  for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', 
                    max_iter=300,n_init = 10, random_state = 0)
    kmeans.fit(X) # fit kmeans to dataset
  # append inertia_ - sum of squre distance of samples to their closest centroid
    wcss.append(kmeans.inertia_) 

  #plot elbow graph
  plt.plot(range(1,11), wcss)
  plt.title("Elbow Graph")
  plt.xlabel("Number of clusters")
  plt.ylabel("WCSS")
  plt.show()

"""### Session Frequncy Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Bearer Id'], "Users' Session Clusters",'MSISDN/Number','Bearer Id')

"""### Duration Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Dur. (ms)'], "Clusters of Duration (ms)",'MSISDN/Number','Duration (ms)')

"""### Min-Max for Non-Normalized Data"""

Cluster(db,['MSISDN/Number', 'Dur. (ms)'], "Clusters of Duration (ms)",'MSISDN/Number','Duration (ms)')

"""### Total Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Bytes'], "Total Bytes Clusters",'MSISDN/Number',"Total Bytes")

"""### Total Social Media Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Social Media (Bytes)'], "Total Social Media Bytes Clusters",'MSISDN/Number',"Total Social Media (Bytes)")

"""### Total Google Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Google (Bytes)'], "Total Google Bytes Clusters",'MSISDN/Number',"Total Google (Bytes)")

"""### Total Email Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Email (Bytes)'], "Total Email Bytes Clusters",'MSISDN/Number',"Total Email (Bytes)")

"""### Total Youtube Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Youtube (Bytes)'], "Total Youtube Bytes Clusters",'MSISDN/Number',"Total Youtube (Bytes)")

"""### Total Netflix Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Netflix (Bytes)'], "Total Netflix Bytes Clusters",'MSISDN/Number',"Total Netflix (Bytes)")

"""### Total Gaming Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Netflix (Bytes)'], "Total Netflix Bytes Clusters",'MSISDN/Number',"Total Netflix (Bytes)")

"""### Total Other Services Bytes Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Other (Bytes)'], "Total Other Services Bytes Clusters",'MSISDN/Number',"Total Other (Bytes)")

"""### Total UL Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Total UL (Bytes)'], "Clusters of Total UL",'MSISDN/Number',"Total UL (Bytes)")

"""### Min-Max for Non-Normalized Data"""

Cluster(db,['MSISDN/Number', 'Total UL (Bytes)'], "Clusters of Total UL",'MSISDN/Number',"Total UL (Bytes)")

"""### Data Scaling and Encoding

#### Drop some columns
"""

print("Numeric scaled data:\n")
db_sklearn.head() # scaled data

columns_drop = ['Start','Start ms', 'End', 'End ms', 'IMSI','IMEI','Last Location Name','Dur. (ms).1']
db_drop_col = db_sklearn.copy()
db_drop_col=db_sklearn.drop(columns_drop, axis=1)
print("Data with some columns dropped: \n")
db_drop_col.head()

db_drop_col.shape

db_encoded = db_drop_col.copy()
lb = LabelEncoder() 
column_encoded = ['Handset Manufacturer', 'Handset Type']


def db_encoding (df):
  for column in column_encoded:
    df[column] = lb.fit_transform(df[column])
  return df

db_encooded = db_encoding(db_encoded)
print("Data with categorical data encoded:\n")
db_encoded.head()

db_user_Duration.head()

"""# User Experience Analysis
* AVG TCP Retrans
* AVG RTT
* AVG TP
* Handset Type

### User Experience Utitlity Functions
"""

# def Aggregate_Sort_db(df, group_by, sort_by,agg_by,agg_func):
#   return df.groupby(group_by).agg(sort_by = (agg_by,agg_func)).sort(by=str(sort_by), ascending = )
# # db_user_TCP_DL_RT_count = db.groupby(["MSISDN/Number"]).agg(TCP_DL_RT_Count = ("TCP DL Retrans. Vol (Bytes)", 'value_counts')).sort_values(by='TCP_DL_RT_Count', ascending = False)#.value_counts(ascending = False) # it also works
# # db_user_TCP_DL_RT_count.head(10)

def group_agg_sort_db(df, group_by, sort_by,agg_by,agg_func):
  return df.groupby(group_by).agg(sort_by = (agg_by,agg_func)).sort_values(by=(agg_by,agg_func), ascending = False)

"""### Total TCP Retransmission"""

db_user_Total_TCPRtrans = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_TCP_RT = ('Total TCP Retrans. (Bytes)', 'sum')).sort_values(by='Total_TCP_RT', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Total_TCPRtrans

"""### Top 10 TCP Retransmissions"""

db_user_Total_TCPRtrans.head(10)

"""### Bottom 10 TCP Retransmissions"""

db_user_Total_TCPRtrans.tail(10)

"""### TCP DL Retramsmission"""

db_user_TCP_DL_RT = db.groupby(["MSISDN/Number"]).agg(TCP_DL_RT = ("TCP DL Retrans. Vol (Bytes)", 'sum')).sort_values(by='TCP_DL_RT', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TCP_DL_RT

"""### Top 10 TCP DL Retramsmission"""

db_user_TCP_DL_RT.head(10)

"""### Bottom 10 TCP DL Retransmissions"""

db_user_TCP_DL_RT.tail(10)

"""### 10 most frequent TCP DL Reteransmissions"""

db_user_TCP_DL_RT_count = db.groupby(["MSISDN/Number"]).agg(TCP_DL_RT_Count = ("TCP DL Retrans. Vol (Bytes)", 'value_counts')).sort_values(by='TCP_DL_RT_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TCP_DL_RT_count.head(10)

"""### TCP UL Retransmission"""

db_user_TCP_UL_RT = db.groupby(["MSISDN/Number"]).agg(TCP_UL_RT = ("TCP UL Retrans. Vol (Bytes)", 'sum')).sort_values(by='TCP_UL_RT', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TCP_UL_RT

"""### Top 10 TCP UL Retransmissions"""

db_user_TCP_UL_RT.head(10)

"""### Bottom 10 TCP UL Retransmissions"""

db_user_TCP_UL_RT.tail(10)

"""### 10 most frequent TCP UL Reteransmissions"""

db_user_TCP_UL_RT_count = db.groupby(["MSISDN/Number"]).agg(TCP_UL_RT_Count = ("TCP UL Retrans. Vol (Bytes)", 'value_counts')).sort_values(by='TCP_UL_RT_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TCP_UL_RT_count.head(10)

"""### Total Avg RTT"""

db_user_Total_RTT = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_RTT = ('Total Avg RTT (ms)', 'sum')).sort_values(by='Total_RTT', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Total_RTT

"""### Top 10 Avg RTT Times"""

db_user_Total_RTT.head(10)

"""### Bottom 10 Avg RTT Times"""

db_user_Total_RTT.tail(10)

"""### Average RTT DL"""

db_user_RTT_DL = db.groupby(["MSISDN/Number"]).agg(AVG_RTT_DL = ("Avg RTT DL (ms)", 'sum')).sort_values(by='AVG_RTT_DL', ascending = False)#.value_counts(ascending = False) # it also works
db_user_RTT_DL

"""### Top 10 Average RTT DL"""

db_user_RTT_DL.head(10)

"""### Bottom 10 Average RTT DL"""

db_user_RTT_DL.tail(10)

"""### 10 most frequnet RTT DL"""

db_user_RTT_DL_count = db.groupby(["MSISDN/Number"]).agg(AVG_RTT_DL_Count = ("Avg RTT DL (ms)", 'value_counts')).sort_values(by='AVG_RTT_DL_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_RTT_DL_count.head(10)

"""### Average RTT UL"""

db_user_RTT_UL = db.groupby(["MSISDN/Number"]).agg(AVG_RTT_UL = ("Avg RTT UL (ms)", 'sum')).sort_values(by='AVG_RTT_UL', ascending = False)#.value_counts(ascending = False) # it also works
db_user_RTT_UL

"""### Top 10 Average RTT UL"""

db_user_RTT_UL.head(10)

"""### Bottom 10 Average RTT UL"""

db_user_RTT_UL.tail(10)

"""### 10 most frequent RTT UL"""

db_user_RTT_UL_count = db.groupby(["MSISDN/Number"]).agg(AVG_RTT_UL_Count = ("Avg RTT UL (ms)", 'value_counts')).sort_values(by='AVG_RTT_UL_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_RTT_UL_count.head(10)

db_totals_col.columns

"""### Total Session Throughput"""

db_user_Total_TP = db_totals_col.groupby(["MSISDN/Number"]).agg(Total_TP = ('Total Avg TP (kbps)', 'sum')).sort_values(by='Total_TP', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Total_TP

"""### Top 10 Session Throughputs"""

db_user_Total_TP.head(10)

"""### Bottom 10 Session Throughputs"""

db_user_Total_TP.tail(10)

"""### Average Session DL Throughput"""

db_user_TP_DL = db.groupby(["MSISDN/Number"]).agg(AVG_TP_DL = ("Avg Bearer TP DL (kbps)", 'sum')).sort_values(by='AVG_TP_DL', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TP_DL

"""### Top 10 Session DL Throughput"""

db_user_TP_DL.head(10)

"""### Bottom 10 Session DL Throughput"""

db_user_TP_DL.tail(10)

"""### 10 most frequent session DL Throughput"""

db_user_TP_DL_count = db.groupby(["MSISDN/Number"]).agg(AVG_TP_DL_Count = ("Avg Bearer TP DL (kbps)", 'value_counts')).sort_values(by='AVG_TP_DL_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TP_DL_count.head(10)

"""### Average Session UL Throughput"""

db_user_TP_UL = db.groupby(["MSISDN/Number"]).agg(AVG_TP_UL = ("Avg Bearer TP UL (kbps)", 'sum')).sort_values(by='AVG_TP_UL', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TP_UL

"""### Top 10 Session UL Throughput"""

db_user_TP_UL.head(10)

"""### Bottom 10 Session UL Throughput"""

db_user_TP_UL.tail(10)

"""### 10 most frequent session UL Throughput"""

db_user_TP_UL_count = db.groupby(["MSISDN/Number"]).agg(AVG_TP_UL_Count = ("Avg Bearer TP UL (kbps)", 'value_counts')).sort_values(by='AVG_TP_UL_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_TP_UL_count.head(10)

"""### Handset Tyep and number of handsets used per user"""

db_user_HandsetT = db.groupby(["MSISDN/Number"]).agg(Handset_Count = ("Handset Type", 'value_counts')).sort_values(by='Handset_Count', ascending = False)#.value_counts(ascending = False) # it also works
db_user_HandsetT

"""### Total TP per Handset"""

db_user_Handset_TP = db_totals_col.groupby(["Handset Type"]).agg(Total_TP_per_Handset = ('Total Avg TP (kbps)', 'sum')).sort_values(by='Total_TP_per_Handset', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Handset_TP

"""### Avg TP DL per Handset Type"""

db_user_Handset_TP_DL = db.groupby(["Handset Type"]).agg(Total_TP_DL_per_Handset = ('Avg Bearer TP DL (kbps)', 'sum')).sort_values(by='Total_TP_DL_per_Handset', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Handset_TP_DL

"""### Avg TP UL per Handset  Type"""

db_user_Handset_TP_UL = db.groupby(["Handset Type"]).agg(Total_TP_UL_per_Handset = ('Avg Bearer TP UL (kbps)', 'sum')).sort_values(by='Total_TP_UL_per_Handset', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Handset_TP_UL

"""### Total TCP Retransmission per Handset Type"""

db_user_Handset_TCP = db_totals_col.groupby(["Handset Type"]).agg(Total_TCP_per_Handset = ('Total TCP Retrans. (Bytes)', 'sum')).sort_values(by='Total_TCP_per_Handset', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Handset_TCP

"""### TCP DL Retransmission per Handset Type

"""

db_user_Handset_TCPRet_DL = db.groupby(["Handset Type"]).agg(TCP_Retrans_DL_per_Handset = ('TCP DL Retrans. Vol (Bytes)', 'sum')).sort_values(by='TCP_Retrans_DL_per_Handset', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Handset_TCPRet_DL

"""### TCP UL Retransmission per Handset Type"""

db_user_Handset_TCPRet_UL = db.groupby(["Handset Type"]).agg(TCP_Retrans_UL_per_Handset = ('TCP UL Retrans. Vol (Bytes)', 'sum')).sort_values(by='TCP_Retrans_UL_per_Handset', ascending = False)#.value_counts(ascending = False) # it also works
db_user_Handset_TCPRet_UL

"""## User Experience Clustering

### Features of Clustering
* TCP retransmission
* Average RTT
* Handset type
* Average throughput (TP)

### Total TCP Retransmission Clusters
"""

Cluster(db_totals_col,['MSISDN/Number', 'Total TCP Retrans. (Bytes)'], "Total TCP Retransmision Clusters",'MSISDN/Number','Total TCP Retrans. (Bytes)')

"""### Total Avg RTT Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Avg RTT (ms)'], "Total Avg RTT Clusters",'MSISDN/Number','Total Avg RTT (ms)')

"""### Total Avg Throughput Clusters"""

Cluster(db_totals_col,['MSISDN/Number', 'Total Avg TP (kbps)'], "Total Avg TP Clusters",'MSISDN/Number','Total Avg TP (kbps)')

"""### TCP DL Retransmission Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)'], "TCP DL Retransmision Clusters",'MSISDN/Number','TCP DL Retrans. Vol (Bytes)')

"""### TCP UL Retransmission Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'TCP UL Retrans. Vol (Bytes)'], "TCP UL Retransmision Clusters",'MSISDN/Number','TCP UL Retrans. Vol (Bytes)')

"""### Avg RTT DL Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Avg RTT DL (ms)'], "Avg RTT DL Clusters",'MSISDN/Number','Avg RTT DL (ms)')

"""### Avg RTT UL Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Avg RTT UL (ms)'], "Avg RTT UL Clusters",'MSISDN/Number','Avg RTT UL (ms)')

"""### Handset Type Clusters"""

Cluster(db_encoded,['MSISDN/Number', 'Handset Type'], "Handset Type Clusters",'MSISDN/Number','Handset Type')

"""### Avg Bearer Throughput(TP) DL Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Avg Bearer TP DL (kbps)'], "AVG TP DL Clusters",'MSISDN/Number','Avg Bearer TP DL (kbps)')

"""### Avg Throughput(TP) UL Clusters"""

Cluster(db_sklearn,['MSISDN/Number', 'Avg Bearer TP UL (kbps)'], "AVG TP UL Clusters",'MSISDN/Number','Avg Bearer TP UL (kbps)')

"""# User Satisfaction Analysis

### Utility Functions
"""

def CalculateUserEnga_Score(df,datapoint_col):

  points = df[[datapoint_col]]
  kmeans_fit = kmeans.fit(points)
  df['cluster'] = kmeans_fit.labels_

  centroids = kmeans_fit.cluster_centers_
  dists = pd.DataFrame(
      sdist.cdist(points, centroids), 
      columns=[f'dist_{i}' for i in range(len(centroids))],index=df.index)
  df = pd.concat([df, dists], axis=1)

  # Engagement score
  df["Engagement_Score"] = df[[f'dist_{i}' for i in range(len(centroids))]].max(axis = 1)
  # df = pd.concat([df,df["Engagement_Score"]],axis = 1)
  return df[["Engagement_Score"]]

  # distance between its own centroid
  
  # # points = df.drop('id', axis=1)
  # points = df[[datapoint_col]]
  # kmeans = cluster.KMeans(n_clusters=3, random_state=0).fit(points)
  # df['cluster'] = kmeans.labels_

  # centroids = kmeans.cluster_centers_
  # dist = sdist.norm(points - centroids[df['cluster']])
  # df['dist'] = dist

  # print(df)

def CalculateUserExpr_Score(df,datapoint_col):

  points = df[[datapoint_col]]
  kmeans_fit = kmeans.fit(points)
  df['cluster'] = kmeans_fit.labels_

  centroids = kmeans_fit.cluster_centers_
  dists = pd.DataFrame(
      sdist.cdist(points, centroids), 
      columns=[f'dist_{i}' for i in range(len(centroids))],index=df.index)
  df = pd.concat([df, dists], axis=1)

  df["Experience_Score"] = df[[f'dist_{i}' for i in range(len(centroids))]].max(axis = 1)
  # df = pd.concat([df,df["Experience_Score"]],axis = 1)
  return df[["Experience_Score"]]

"""### Duration Engagement Score"""

db_engag = db_totals_col.copy()

engage_score_dur = CalculateUserEnga_Score(db_totals_col, "Dur. (ms)")
db_engag["Engagement_Score_Dur"] = engage_score_dur["Engagement_Score"]
db_engag[['MSISDN/Number', 'Engagement_Score_Dur']]

"""### Total Bytes Engagement Score"""

engage_score_total = CalculateUserEnga_Score(db_totals_col, 'Total Bytes')
db_engag["Engagement_Score_Total"] = engage_score_total["Engagement_Score"]
db_engag[['MSISDN/Number', 'Engagement_Score_Total']]#.head(10)

"""# Expereince Score"""

db_exper = db_totals_col.copy()

"""### Total TCP Retransmission Experience Score"""

exper_score_total_TCP = CalculateUserExpr_Score(db_totals_col, 'Total TCP Retrans. (Bytes)')
db_exper["Experience_Score_Total_TCP"] = exper_score_total_TCP["Experience_Score"]
db_exper[['MSISDN/Number', 'Experience_Score_Total_TCP']]#.head(10)

"""### Total RTT Experience Score"""

exper_score_total_RTT = CalculateUserExpr_Score(db_totals_col, 'Total Avg RTT (ms)')
db_exper["Experience_Score_Total_RTT"] = exper_score_total_RTT["Experience_Score"]
db_exper[['MSISDN/Number', 'Experience_Score_Total_RTT']]#.head(10)

"""### Total TP Experience Score"""

exper_score_total_TP = CalculateUserExpr_Score(db_totals_col, 'Total Avg TP (kbps)')
db_exper["Experience_Score_Total_TP"] = exper_score_total_TP["Experience_Score"]
db_exper[['MSISDN/Number', 'Experience_Score_Total_TP']]#.head(10)

"""# Satisfaction Score

### Engagement Satisfaction Score
"""

satisf_score_engag = (db_engag['Engagement_Score_Dur'] + db_engag['Engagement_Score_Total'])/2.0
db_engag["Engagement_Satisf_Score"] = satisf_score_engag
db_engag[['MSISDN/Number', 'Engagement_Satisf_Score']].head(10)

"""### Experience Satisfaction Score"""

satisf_score_exper= (db_exper['Experience_Score_Total_TCP'] + db_exper['Experience_Score_Total_RTT'] + db_exper['Experience_Score_Total_TP'])/3.0
db_exper["Experience_Satisf_Score"] = satisf_score_exper
# db_exper.rename(columns = {'Satisf_Score':'Experience_Satisf_Score'})
db_exper[['MSISDN/Number', 'Experience_Satisf_Score']].head(10)

"""## Engagement Satisfaction Cluster"""

Cluster(db_engag,['MSISDN/Number', 'Engagement_Satisf_Score'], "Engagement Satisfaction Clusters",'MSISDN/Number','Engagement_Satisf_Score')

"""## Experience Satisfaction Cluster"""

Cluster(db_exper,['MSISDN/Number', 'Experience_Satisf_Score'], "Experience Satisfaction Clusters",'MSISDN/Number','Experience_Satisf_Score')

"""## Engagement Clusters"""

Cluster(db_engag,['MSISDN/Number', 'Engagement_Score_Total'], "Total Bytes Engagement Clusters",'MSISDN/Number','Engagement_Score_Total')

"""## Experience Cluster"""

Cluster(db_exper,['MSISDN/Number', 'Experience_Score_Total_TP'], "Total TP Experience Clusters",'MSISDN/Number','Experience_Score_Total_TP')
'Experience_Score_Total_TP'

"""### MySQL Table Columns"""

# eng_score_total= db_engag['Engagement_Score_Dur'] + db_engag['Engagement_Score_Total']
# exper_score_total= db_exper['Experience_Score_Total_TCP'] + db_exper['Experience_Score_Total_RTT'] + db_exper['Experience_Score_Total_TP']
db_1 = db_engag[['MSISDN/Number','Engagement_Score_Dur', 'Engagement_Score_Total','Engagement_Satisf_Score']]
db_2 = db_exper[['Experience_Score_Total_TCP', 'Experience_Score_Total_RTT','Experience_Score_Total_TP', 'Experience_Satisf_Score']]
db_mysql = pd.concat([db_1,db_2], axis = 1)
db_mysql
# db_mysql[["MSISDN_Number", "Engagement_Score", "Experience_Score", "Satisfaction_Score"]] =

db_mysql.to_csv("/content/drive/MyDrive/Colab Notebooks/data/User_Scores.csv",
                header = True, index = False)

db_mysql.columns
# %%
